# Dequantize MLX Model

Possible use-cases:

* Converting a 4-bit quantized model (without QLoRA/LoRA fine-tuning) to a GGUF format for inference using ollama or LM Studio
